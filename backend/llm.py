# backend/llm.py
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain

# Load environment variables from .env file
load_dotenv()

# Ensure the Google API key is set
if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("GOOGLE_API_KEY environment variable not set. Please create a .env file and add it.")

# --- LLM and Conversation Chain Setup ---

# Initialize the Gemini model
# We use "gemini-1.5-flash" for a balance of performance and speed.
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)

# Set up conversation memory to hold the last 4 exchanges (k=4)
# This helps the model remember the context of the recent conversation.
memory = ConversationBufferWindowMemory(k=4)

# Create the conversation chain, linking the LLM and the memory
conversation_chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=False  # Set to True to see chain activity in the console
)

def get_chat_response(user_input):
    """
    Gets a response from the LLM for a given user input.

    Args:
        user_input (str): The text input from the user.

    Returns:
        str: The response generated by the language model.
    """
    # The `predict` method runs the chain with the user's input
    # and automatically handles loading/saving messages to memory.
    response = conversation_chain.predict(input=user_input)
    return response