# backend/llm.py
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain

# Load environment variables from .env file
load_dotenv()

# Ensure the Google API key is set
if "GOOGLE_API_KEY" not in os.environ:
    raise ValueError("GOOGLE_API_KEY environment variable not set. Please create a .env file and add it.")

# --- LLM and Conversation Chain Setup ---

# Initialize the Gemini model
# We use "gemini-1.5-flash" for a balance of performance and speed.
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)

# Set up conversation memory to hold the last 4 exchanges (k=4)
# This helps the model remember the context of the recent conversation.
memory = ConversationBufferWindowMemory(k=4)

# Create the conversation chain, linking the LLM and the memory
conversation_chain = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=False  # Set to True to see chain activity in the console
)

MONTH_NAMES = {
    1: "January", 2: "February", 3: "March", 4: "April", 5: "May", 6: "June",
    7: "July", 8: "August", 9: "September", 10: "October", 11: "November", 12: "December"
}

def _build_context_prompt(financial_context):
    """Builds a detailed string prompt from the financial data."""
    year = financial_context.get("year")
    month_num = financial_context.get("month")
    month_name = MONTH_NAMES.get(month_num, "N/A")
    budget = financial_context.get("budget") or {}
    costs = financial_context.get("costs") or []

    prompt_parts = [
        f"You are a helpful and friendly financial assistant. Analyze the user's financial data for **{month_name} {year}** provided below to answer their question.",
        "Your answers should be direct, insightful, and based *only* on the provided data. Do not make up information.",
        "\n--- FINANCIAL DATA SUMMARY ---\n"
    ]

    # Budget Information
    salary = budget.get('salary', 0)
    savings_goal = budget.get('savings_goal', 0)
    prompt_parts.append(f"**Budget:**")
    prompt_parts.append(f"- Monthly Income: ${salary:,.2f}")
    prompt_parts.append(f"- Monthly Savings Goal: ${savings_goal:,.2f}\n")

    # Expenses Information
    fixed_costs = [c for c in costs if c['cost_type'] == 'fixed']
    variable_costs = [c for c in costs if c['cost_type'] == 'variable']
    total_fixed = sum(c['amount'] for c in fixed_costs)
    total_variable = sum(c['amount'] for c in variable_costs)
    total_spending = total_fixed + total_variable

    prompt_parts.append(f"**Spending:**")
    prompt_parts.append(f"- Total Fixed Costs: ${total_fixed:,.2f}")
    prompt_parts.append(f"- Total Variable Costs: ${total_variable:,.2f}")
    prompt_parts.append(f"- **Grand Total Spending:** ${total_spending:,.2f}\n")

    # Detailed Expense Lists
    if fixed_costs:
        prompt_parts.append("Fixed Expenses List:")
        for cost in fixed_costs:
            prompt_parts.append(f"  - {cost['name']}: ${cost['amount']:,.2f}")
    
    if variable_costs:
        prompt_parts.append("\nVariable Expenses List:")
        for cost in variable_costs:
            prompt_parts.append(f"  - {cost['name']}: ${cost['amount']:,.2f}")

    prompt_parts.append("\n------------------------------\n")
    return "\n".join(prompt_parts)


def get_chat_response(user_input, financial_context):
    """
    Gets a response from the LLM, including financial data as context.

    Args:
        user_input (str): The text input from the user.
        financial_context (dict): A dictionary with the user's financial data.

    Returns:
        str: The response generated by the language model.
    """
    # Build the full prompt with context + the user's question
    context_prompt = _build_context_prompt(financial_context)
    full_prompt = f"{context_prompt}User Question: {user_input}"
    
    # The `predict` method runs the chain with the full prompt
    # and automatically handles loading/saving messages to memory.
    response = conversation_chain.predict(input=full_prompt)
    return response